{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pytorch Forecasting | Temporal Fusion Transformer](https://www.kaggle.com/code/crustacean/pytorch-forecasting-temporal-fusion-transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/sales_train.csv')\n",
    "test = pd.read_csv('../../data/sales_test.csv')\n",
    "solution = pd.read_csv('../../data/solution.csv')\n",
    "inv = pd.read_csv('../../data/inventory.csv')\n",
    "cle = pd.read_csv('../../data/calendar.csv')\n",
    "test_weights = pd.read_csv('../../data/test_weights.csv')\n",
    "train = train.merge(inv,on=['warehouse','unique_id'],how='left')\n",
    "train['unique_id_and_name'] = train['unique_id'].astype(str) + '_' + train['name']\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = train.sales.isna().sum()\n",
    "print(f\"Number of NaN entries in 'sales': {nan_count}\")\n",
    "train = train.dropna(subset=['sales'])\n",
    "nan_count_after = train.sales.isna().sum()\n",
    "print(f\"Number of NaN entries in 'sales' after dropping: {nan_count_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time index\n",
    "min_time_idx = train['date'].min()\n",
    "train['time_idx'] = (train['date'] - min_time_idx).dt.days\n",
    "train['unique_id'] = train['unique_id'].astype(str).astype('category')\n",
    "test['time_idx'] = (test['date'] - min_time_idx).dt.days\n",
    "test['unique_id'] = test['unique_id'].astype(str).astype('category')\n",
    "test['sales'] = 0.0\n",
    "test = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.time_idx.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 28\n",
    "max_encoder_length = 28\n",
    "training_cutoff = train[\"time_idx\"].max() # - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train, #[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"sales\",\n",
    "    group_ids=[\"unique_id\"],\n",
    "    min_encoder_length=1, # max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\n",
    "        \"unique_id\",\n",
    "        'warehouse',\n",
    "    ],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\n",
    "        \"time_idx\",\n",
    "        'total_orders',\n",
    "        'sell_price_main',\n",
    "        'type_0_discount',\n",
    "        'type_1_discount',\n",
    "        'type_2_discount',\n",
    "        'type_3_discount',\n",
    "        'type_4_discount',\n",
    "        'type_5_discount',\n",
    "        'type_6_discount',\n",
    "    ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        'sales',\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"unique_id\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, train, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0) # batch_size=batch_size * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    loss=MAE(),\n",
    "    log_interval=10,  \n",
    "    optimizer=\"adamw\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs= 200, \n",
    "    accelerator=\"gpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = best_tft.predict(test, \n",
    "                                   mode=\"raw\", \n",
    "                                   return_x=True, \n",
    "                                   return_index=True,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    best_tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index = raw_predictions.index\n",
    "preds = raw_predictions.output.prediction.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index.time_idx.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = solution[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution[['unique_id', 'date']] = solution['id'].str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution['date'] = pd.to_datetime(solution['date'])\n",
    "start_date = train['date'].min()\n",
    "solution['time_idx'] = (solution['date'] - start_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.squeeze(-1).numpy()\n",
    "horizon = preds.shape[1]\n",
    "pred_index[\"forecast_idx\"] = range(len(pred_index)) \n",
    "expanded_forecasts = []\n",
    "\n",
    "for idx, row in pred_index.iterrows():\n",
    "    start_time_idx = row[\"time_idx\"]\n",
    "    unique_id = row[\"unique_id\"]\n",
    "    forecast_values = preds[idx]\n",
    "    time_indices = np.arange(start_time_idx, start_time_idx + horizon)\n",
    "    \n",
    "    expanded_forecasts.append(pd.DataFrame({\n",
    "        \"time_idx\": time_indices,\n",
    "        \"unique_id\": unique_id,\n",
    "        \"sales_hat\": forecast_values\n",
    "    }))\n",
    "\n",
    "expanded_forecasts_df = pd.concat(expanded_forecasts, ignore_index=True)\n",
    "\n",
    "solution = solution.merge(expanded_forecasts_df, on=[\"unique_id\", \"time_idx\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution[solution['sales_hat'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
